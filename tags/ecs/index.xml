<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ECS on Go slowly</title>
    <link>https://manhtai.github.io/tags/ecs/</link>
    <description>Recent content in ECS on Go slowly</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Aug 2021 14:04:41 +0700</lastBuildDate>
    
        <atom:link href="https://manhtai.github.io/tags/ecs/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Distribute workload between ECS tasks</title>
      <link>https://manhtai.github.io/posts/distribute-workload-in-ecs-tasks/</link>
      <pubDate>Thu, 19 Aug 2021 14:04:41 +0700</pubDate>
      
      <guid>https://manhtai.github.io/posts/distribute-workload-in-ecs-tasks/</guid>
      <description>&lt;p&gt;If your ECS tasks are receiving traffic from a load balancer then the workload
will be equally distributed between them. How about when we are using ECS
tasks as a worker farm to handle long running jobs? And say, we want some
workers to work on some partitions of the data but not all of them? Then each
ECS task must know their identity and the number of tasks that belong to the
same service as well.&lt;/p&gt;
&lt;h2 id=&#34;1-get-task-arn&#34;&gt;1. Get task ARN&lt;/h2&gt;
&lt;p&gt;With &lt;code&gt;${ECS_CONTAINER_METADATA_URI_V4}/task&lt;/code&gt; endpoint, we can get the task ARN
and metadata about its cluster and family.&lt;/p&gt;
&lt;p&gt;After sending a GET request from our container, we got:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;Cluster&amp;quot;: &amp;quot;default&amp;quot;,
    &amp;quot;TaskARN&amp;quot;: &amp;quot;arn:aws:ecs:us-west-2:111122223333:task/default/158d1c8083dd49d6b527399fd6414f5c&amp;quot;,
    &amp;quot;Family&amp;quot;: &amp;quot;curltest&amp;quot;,
    ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This request doesn&amp;rsquo;t require any authentication at all, as long as we send it
from our ECS task.&lt;/p&gt;
&lt;h2 id=&#34;2-list-all-tasks-in-the-same-service&#34;&gt;2. List all tasks in the same service&lt;/h2&gt;
&lt;p&gt;With &lt;code&gt;Cluster&lt;/code&gt; and &lt;code&gt;Family&lt;/code&gt; of a task, we can list all running tasks in
a service using ECS API, in this example we will use Go SDK though:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Go&#34; data-lang=&#34;Go&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ecsClient&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;ListTasks&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;context&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;TODO&lt;/span&gt;(), &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;ecs&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;ListTasksInput&lt;/span&gt;{
	&lt;span style=&#34;color:#a6e22e&#34;&gt;Cluster&lt;/span&gt;:       &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,
	&lt;span style=&#34;color:#a6e22e&#34;&gt;Family&lt;/span&gt;:        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;curltest&amp;#34;&lt;/span&gt;,
	&lt;span style=&#34;color:#a6e22e&#34;&gt;DesiredStatus&lt;/span&gt;: &lt;span style=&#34;color:#a6e22e&#34;&gt;types&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;DesiredStatusRunning&lt;/span&gt;,
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;list.TaskArns&lt;/code&gt; contains all ARN of tasks in the service, including the task
making the request. This request does require authentication nevertheless.&lt;/p&gt;
&lt;h2 id=&#34;3-distribute-the-workload&#34;&gt;3. Distribute the workload&lt;/h2&gt;
&lt;p&gt;Now we know how many tasks we got, the problem becomes easy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Memory Leak in Celery</title>
      <link>https://manhtai.github.io/posts/memory-leak-in-celery/</link>
      <pubDate>Sun, 15 Apr 2018 10:51:06 +0700</pubDate>
      
      <guid>https://manhtai.github.io/posts/memory-leak-in-celery/</guid>
      <description>&lt;p&gt;Turn out Celery has &lt;a href=&#34;https://github.com/celery/celery/issues/1427&#34;&gt;some memory leaks&lt;/a&gt;. We don&amp;rsquo;t know that beforehand.
After deploying some Celery servers using AWS ECS we notice that all Celery
tasks will consume most of the server memory and then become idle.&lt;/p&gt;
&lt;p&gt;My first attempt was set &lt;a href=&#34;https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html&#34;&gt;hard limit&lt;/a&gt; for container memory to 1GiB. And
guess what? Celery will consume 99.9% of that limit then become idle after
some times. It&amp;rsquo;s good for the server but doesn&amp;rsquo;t solve our problem.&lt;/p&gt;
&lt;p&gt;My second attempt was set &lt;code&gt;CELERYD_TASK_TIME_LIMIT&lt;/code&gt; to 300, so celery tasks
will be killed after 5 minutes no matter what. This time Celery continue to
take memory percentage as much as it can and then become inactive, but after
5 minutes it kills all the tasks to release memory and then back to work
normally.&lt;/p&gt;
&lt;p&gt;I thought it worked, but it didn&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;After running for some periods, Celery still hung. So it&amp;rsquo;s not due to the leak
anymore. Continue digging around, I found out the main reason Celery hangs is
due to &lt;a href=&#34;https://github.com/celery/celery/issues/2917&#34;&gt;some thread locks&lt;/a&gt; caused by &lt;a href=&#34;https://github.com/neo4j/neo4j-python-driver&#34;&gt;neo4j python driver&lt;/a&gt;. And that can
only be solved completely by changing the way neo4j driver save &amp;amp; fetch data
to async, which is still &lt;a href=&#34;https://github.com/neo4j/neo4j-python-driver/issues/180&#34;&gt;an open issue&lt;/a&gt; on GitHub. Although people gave
some temporary solutions to the problem, it&amp;rsquo;s only apply for Python3, and our
project is still Python2. Hence, a &lt;a href=&#34;https://manhtai.github.io/posts/python-2to3-transition&#34;&gt;transition&lt;/a&gt; from Python2 to Python3 is
needed.&lt;/p&gt;
&lt;p&gt;In the mean time, I set up a cronjob to restart Celery after some times to
remove the lock.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>